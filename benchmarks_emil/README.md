# Benchmarking Suite

This directory contains scripts for running and analyzing benchmarks for the search algorithms.

## 1. Running Benchmarks (`run_benchmarks_hyperfine.py`)

The `run_benchmarks_hyperfine.py` script is used to execute benchmarks for specified levels using `hyperfine` for timing and also captures application-specific metrics (like solution length).

### Key Features:
- Runs specified search strategies on given level files.
- Uses `hyperfine` for accurate timing of command execution.
- Captures application metrics output by the C++ search client (e.g., `SolutionLength`, `ExpandedNodes`).
- Supports timeouts for both the initial direct run (to get app metrics) and for each `hyperfine` measured run per strategy.
- Outputs results to a JSON file, typically in the `benchmark_results/` subdirectory.
- Live progress display in the terminal.

### Basic Usage:

To run benchmarks for a single level:
```bash
uv run python benchmarks/run_benchmarks_hyperfine.py path/to/your/level.lvl [options]
```

To run benchmarks for multiple levels (e.g., all levels in a directory):
```bash
uv run python benchmarks/run_benchmarks_hyperfine.py levels/your_level_dir/*.lvl [options]
```

### Common Options:
-   `--output-dir DIR_PATH`: Specifies the directory where the JSON output file (e.g., `hyperfine_run_YYYYMMDD_HHMMSS.json`) will be saved. Default: `benchmarks/benchmark_results/`.
-   `--strategies-to-test "strat1" "strat2"`: A list of strategies to test (e.g., `"-s bfs -hc zero"`, `"-s astar -hc mds"`). Defaults to a predefined list in the script.
-   `--hyperfine-options OPTS_STRING`: A string of options to pass directly to `hyperfine` (e.g., `'--warmup 3 --runs 10'`).
-   `--hyperfine-timeout-per-strategy SECONDS`: Timeout in seconds for each `hyperfine` run of a single strategy on a single level.
-   `--direct-run-timeout-per-level SECONDS`: Timeout for the initial non-hyperfine run of all strategies on a level (used to quickly gather app metrics).
-   `--config-file path/to/benchmarks.json`: Path to a JSON configuration file that can define levels, strategies, and timeouts.

**Example:** Run all levels in `levels/comp/` with 1 warmup and 3 runs for `hyperfine`, a 60s timeout per strategy, and output to `benchmarks/benchmark_results/`:
```bash
uv run python benchmarks/run_benchmarks_hyperfine.py levels/comp/*.lvl --hyperfine-options '--warmup 1 --runs 3' --hyperfine-timeout-per-strategy 60 --output-dir benchmarks/benchmark_results/
```

### Application Metrics:
The script parses lines starting with `#` from the C++ search client's `stdout` as key-value pairs. For example, if the C++ program prints:
```
#SolutionLength,ExpandedNodes,GeneratedNodes
#42,1000,2000
```
The script will store `{"solutionlength": 42, "expandednodes": 1000, "generatednodes": 2000}` in the `app_metrics` for that run. Note that keys are lowercased.

## 2. Comparing Benchmark Results (`compare_hyperfine_results.py`)

The `compare_hyperfine_results.py` script is used to display and compare the JSON output files generated by `run_benchmarks_hyperfine.py`.

### Key Features:
-   Displays results from a single benchmark file in a structured table format.
-   Compares results from multiple benchmark files side-by-side.
-   Highlights best performers for mean time, steps, and standard deviation for each level.
-   Calculates percentage difference in mean times when comparing exactly two files.
-   Can automatically find and load the latest benchmark file from a specified directory.
-   Supports output to the console, plain text files, or HTML files.

### Usage:

To view results from a single JSON file:
```bash
uv run python benchmarks/compare_hyperfine_results.py path/to/benchmark_file.json [options]
```

To compare multiple JSON files:
```bash
uv run python benchmarks/compare_hyperfine_results.py file1.json file2.json ... [options]
```

To automatically use the latest benchmark run from `benchmarks/benchmark_results/`:
```bash
uv run python benchmarks/compare_hyperfine_results.py --latest-in benchmarks/benchmark_results/ [options]
```

### Common Options:
-   `--latest-in DIRECTORY`: Directory to search for the latest benchmark file (e.g., `hyperfine_run_YYYYMMDD_HHMMSS.json`).
-   `-o FILE_PATH`, `--output-file FILE_PATH`: Path to save the comparison output.
    -   If `FILE_PATH` ends with `.html`, the output will be a formatted HTML page.
    -   Otherwise, it will be plain text.
    -   If not provided, output goes to the console.

### Interpreting Output:
-   **Status**: Indicates if the run was successful, timed out, or encountered an error.
-   **Steps**: Shows the value of the application metric defined by `APP_METRIC_FOR_STEPS` (currently `solutionlength`) in the script. This is sourced from the `SolutionLength` field (case-insensitive) in the C++ output's `#` comments.
-   **Mean (s), Median (s), StdDev (s), Min (s), Max (s)**: These are timing statistics from `hyperfine`.
-   **Footer Summary (Single File View)**: For each level, separate lines in the footer will indicate:
    -   `Fastest (Mean Time)`: The best mean time and the strategy/strategies achieving it.
    -   `Fewest Steps`: The minimum steps and the corresponding strategy/strategies.
    -   `Lowest StdDev`: The lowest standard deviation and the corresponding strategy/strategies.
-   **Multi-File Comparison**:
    -   Shows mean time, standard deviation, and steps for each strategy across different benchmark runs.
    -   Highlights the best mean time and fewest steps for each strategy row across the files.
    -   If exactly two files are compared, a "Mean Diff %" column shows the percentage change.

---
This README provides a basic guide. For more detailed options and behaviors, refer to the help messages of the scripts using the `-h` or `--help` flag.
